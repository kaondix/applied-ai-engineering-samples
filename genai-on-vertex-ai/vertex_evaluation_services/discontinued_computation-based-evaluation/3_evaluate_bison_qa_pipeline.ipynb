{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|----------|-------------|\n",
    "| Author(s)   | Renato Leite (renatoleite@), Egon Soares (egon@) |\n",
    "| Last updated | 07/09/2024 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============== DISCONTINUED ==========================\n",
    "# LLM Evaluation Workflow for a Classification Task using Text-Bison and Vertex AI Pipelines\n",
    "\n",
    "In this notebook, we will explore various aspects related to running the Vertex LLM evaluation pipeline. Our journey will encompass the following key stages:\n",
    "\n",
    "1. **Data Preparation**: Before we dive into the evaluation process, we'll ensure that our data is prepped and ready to be fed into the pipeline.\n",
    "\n",
    "2. **Evaluation with Model text-bison@001**: We will execute the evaluation phase using the foundational model, specifically text-bison@001. To initiate the evaluation job, we will utilize the open-source pipeline definition.\n",
    "\n",
    "3. **Metric Retrieval and Visualization**: Once we've run the evaluation, we'll extract all the valuable metrics generated as artifacts by the pipeline. These metrics will be uploaded to an ExperimentsRun and will be able to visualize inside the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/notebook3.png\" style=\"width:800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Vertex AI LLM SDK (Private Preview)\n",
    "! pip install -U google-cloud-aiplatform\n",
    "! pip install -U google-cloud-pipeline-components\n",
    "! pip install \"shapely<2.0.0\"\n",
    "\n",
    "# Install HuggingFace Datasets\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL (if you are using Colab, restart the Kernel at this point, uncommend and execute the following code)\n",
    "# from google.colab import auth as google_auth\n",
    "# google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python packages and define project variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "import uuid\n",
    "\n",
    "from datasets import load_dataset\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google_cloud_pipeline_components.preview.model_evaluation import evaluation_llm_classification_pipeline\n",
    "from kfp import compiler\n",
    "from kfp import dsl\n",
    "from vertexai.preview.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the values of the variables below according to your project specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project variables\n",
    "PROJECT_ID = \"<YOUR PROJECT ID>\"\n",
    "\n",
    "ENDPOINT_LOCATION = \"us-central1\"\n",
    "STAGING_BUCKET = \"gs://<YOUR BUCKET NAME>\"    # Same location as your ENDPOINT_LOCATION\n",
    "\n",
    "storage_client = storage.Client()\n",
    "vertexai.init(project=PROJECT_ID, location=ENDPOINT_LOCATION, staging_bucket=STAGING_BUCKET)\n",
    "aiplatform.init(project=PROJECT_ID, location=ENDPOINT_LOCATION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset for evaluation\n",
    "\n",
    "In this lab, you are going to evaluate the **text-bison** foundation model for a single label text classification task. You are going to use the `dair-ai/emotion` dataset from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from HuggingFace\n",
    "dataset = load_dataset('dair-ai/emotion', split='test[:5%]')\n",
    "print('Dataset structure:\\n', dataset)\n",
    "print('Sample:\\n', dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation dataset used for model evaluation includes **prompt** and **ground truth** pairs that align with the task that you want to evaluate. Your dataset must include a minimum of one prompt and ground truth pair, but we recommend at least 10 pairs for meaningful metrics. Generally speaking, the more examples you give, the more meaningful the results.\n",
    "\n",
    "The dataset can be in 2 different formats:\n",
    " - Pandas Dataframe\n",
    " - JSONL file on Google Cloud Storage\n",
    "\n",
    "Next we will demonstrate both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear',\n",
    "    5: 'surprise'\n",
    "}\n",
    "\n",
    "instructions = f'''Classify the following text into one of the following classes: \n",
    "[{', '.join(class_labels.values())}]\n",
    "Text:\n",
    "'''\n",
    "\n",
    "def add_instructions(example, instructions):\n",
    "    example[\"prompt\"] = f'{instructions}{example[\"text\"]}'\n",
    "    example[\"ground_truth\"] = class_labels[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "eval_dataset = dataset.map(lambda x: add_instructions(x, instructions)).remove_columns(['text', 'label'])\n",
    "\n",
    "print(eval_dataset)\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataset split to GCS\n",
    "jsonl_filename = 'emotions-eval.jsonl'\n",
    "gcs_uri = f'{STAGING_BUCKET}/{jsonl_filename}'\n",
    "eval_dataset.to_json(jsonl_filename)\n",
    "\n",
    "# Copy file to GCS\n",
    "!gsutil cp {jsonl_filename} {gcs_uri}\n",
    "\n",
    "# List GCS bucket to verify the file was copied successfully\n",
    "!gsutil ls {STAGING_BUCKET}/*.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Vertex AI LLM Model Evaluation job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Simple evaluation pipeline submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_pipeline_path = 'classification_pipeline.json'\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=evaluation_llm_classification_pipeline,\n",
    "    package_path=classification_pipeline_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = TextGenerationModel.from_pretrained('text-bison@001')\n",
    "model_name = base_model._model_resource_name\n",
    "\n",
    "job_id = \"base-model-evaluation-{}\".format(uuid.uuid4())\n",
    "experiment_name = 'tweet-emotion-classification'\n",
    "\n",
    "target_field_name='ground_truth'\n",
    "evaluation_class_labels=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"location\": ENDPOINT_LOCATION,\n",
    "    \"batch_predict_gcs_destination_output_uri\": f'{STAGING_BUCKET}/output',\n",
    "    \"evaluation_class_labels\": evaluation_class_labels,\n",
    "    \"batch_predict_gcs_source_uris\": [gcs_uri],\n",
    "    \"target_field_name\": 'ground_truth',\n",
    "    \"model_name\": model_name\n",
    "}\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    template_path=classification_pipeline_path,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=STAGING_BUCKET,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.submit(experiment=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Evaluation pipeline with custom visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Markdown\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'google_cloud_pipeline_components',  \n",
    "        'google-cloud-storage',\n",
    "        'pandas']\n",
    ")\n",
    "def record_metrics_component(\n",
    "    evaluation_class_labels: list,\n",
    "    evaluation_metrics: Input[artifact_types.ClassificationMetrics],\n",
    "    confusion_artifact: Output[dsl.ClassificationMetrics],\n",
    "    classification_artifact: Output[Markdown],\n",
    "    raw_metrics: Output[dsl.Metrics]\n",
    "):\n",
    "    import json\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Read metrics content from GCS\n",
    "    def get_metrics_blob(metrics_uri):\n",
    "        splits = metrics_uri.split(\"/\")\n",
    "        bucket_name = splits[2]\n",
    "        blob_name = '/'.join(splits[3:])\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        with blob.open(\"r\") as f:\n",
    "            return json.loads(f.read())\n",
    "\n",
    "    def get_confusion_matrix(overall_metrics):\n",
    "        confusion_matrix = []\n",
    "        for slice_metric in overall_metrics['slicedMetrics']:\n",
    "            if 'value' in slice_metric['singleOutputSlicingSpec']:\n",
    "                continue\n",
    "            for row in slice_metric['metrics']['classification']['confusionMatrix']['rows']:\n",
    "                confusion_matrix.append(row['dataItemCounts'])\n",
    "        return confusion_matrix\n",
    "\n",
    "    # Define the function to print classification metrics\n",
    "    def get_classification_metrics(overall_metrics):\n",
    "        all_metrics = overall_metrics['slicedMetrics']\n",
    "        metric_names = [\"Metric Slice\", \"auPrc\", \"auRoc\", \"logLoss\"]\n",
    "        f1_metrics = [\"f1Score\"]\n",
    "        aggregated_f1_metrics = [\"f1ScoreMicro\", \"f1ScoreMacro\"]\n",
    "        table = [metric_names + f1_metrics + aggregated_f1_metrics]\n",
    "        for metrics in all_metrics:\n",
    "            classification_metric = metrics['metrics']['classification']\n",
    "            slice_name = \"class - \" + metrics['singleOutputSlicingSpec']['value'] if 'value' in metrics['singleOutputSlicingSpec'] else \"Overall\"\n",
    "            slice_metric_values = [slice_name]\n",
    "            slice_metric_values.extend(\n",
    "                [classification_metric.get(metric_name, 0) \n",
    "                 for metric_name in metric_names[1:]])\n",
    "            slice_metric_values.extend(\n",
    "                [classification_metric['confidenceMetrics'][0].get(metric_name, 0) \n",
    "                 for metric_name in f1_metrics])\n",
    "            slice_metric_values.extend(\n",
    "                [classification_metric['confidenceMetrics'][0].get(metric_name, 'n/a') \n",
    "                 for metric_name in aggregated_f1_metrics])\n",
    "            table.append(slice_metric_values)\n",
    "        return table\n",
    "\n",
    "    # Log Confusion Matrix artifact\n",
    "    overall_metrics = get_metrics_blob(metrics_uri=evaluation_metrics.uri)\n",
    "    confusion_matrix = get_confusion_matrix(overall_metrics)\n",
    "    evaluation_class_labels.append('UNKNOWN')\n",
    "    confusion_artifact.log_confusion_matrix(\n",
    "        categories=evaluation_class_labels,\n",
    "        matrix=confusion_matrix\n",
    "    )\n",
    "\n",
    "    # Log Classification metrics\n",
    "    metrics_table = get_classification_metrics(overall_metrics)\n",
    "    markdown_content = pd.DataFrame(metrics_table).to_markdown()\n",
    "    with open(classification_artifact.path, 'w') as fp:\n",
    "        fp.write(markdown_content)\n",
    "\n",
    "    # Log Raw metrics\n",
    "    raw_metrics.log_metric(\n",
    "        metric='f1Score',\n",
    "        value=metrics_table[1][4]\n",
    "    )\n",
    "    \n",
    "    # Log Raw metrics\n",
    "    raw_metrics.log_metric(\n",
    "        metric='f1ScoreMicro',\n",
    "        value=metrics_table[1][5]\n",
    "    )\n",
    "    \n",
    "    # Log Raw metrics\n",
    "    raw_metrics.log_metric(\n",
    "        metric='f1ScoreMacro',\n",
    "        value=metrics_table[1][6]\n",
    "    )\n",
    "\n",
    "\n",
    "@dsl.pipeline\n",
    "def custom_evaluation_pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    batch_predict_gcs_destination_output_uri: str,\n",
    "    evaluation_class_labels: list,\n",
    "    batch_predict_gcs_source_uris: list,\n",
    "    model_name: str, \n",
    "    target_field_name: str\n",
    "):\n",
    "    eval_pipeline = evaluation_llm_classification_pipeline(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        batch_predict_gcs_destination_output_uri=batch_predict_gcs_destination_output_uri,\n",
    "        evaluation_class_labels=evaluation_class_labels,\n",
    "        batch_predict_gcs_source_uris=batch_predict_gcs_source_uris,\n",
    "        target_field_name=target_field_name,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "    record_metrics_component(\n",
    "        evaluation_class_labels=evaluation_class_labels,\n",
    "        evaluation_metrics=eval_pipeline.outputs['evaluation_metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = TextGenerationModel.from_pretrained('text-bison@001')\n",
    "model_name = base_model._model_resource_name\n",
    "\n",
    "job_id = \"notebooks3-custom-model-evaluation-{}\".format(uuid.uuid4())\n",
    "experiment_name = 'tweet-emotion-classification'\n",
    "\n",
    "target_field_name='ground_truth'\n",
    "evaluation_class_labels=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_classification_pipeline_path = 'custom_evaluation_pipeline.json'\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=custom_evaluation_pipeline,\n",
    "    package_path=custom_classification_pipeline_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"location\": ENDPOINT_LOCATION,\n",
    "    \"batch_predict_gcs_destination_output_uri\": f'{STAGING_BUCKET}/output',\n",
    "    \"evaluation_class_labels\": evaluation_class_labels,\n",
    "    \"batch_predict_gcs_source_uris\": [gcs_uri],\n",
    "    \"target_field_name\": 'ground_truth',\n",
    "    \"model_name\": model_name,\n",
    "}\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    template_path=custom_classification_pipeline_path,\n",
    "    pipeline_root=STAGING_BUCKET,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.submit(experiment=experiment_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
