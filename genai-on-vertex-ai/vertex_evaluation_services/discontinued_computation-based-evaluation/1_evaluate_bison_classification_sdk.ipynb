{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|----------|-------------|\n",
    "| Author(s)   | Renato Leite (renatoleite@), Egon Soares (egon@) |\n",
    "| Last updated | 07/09/2024 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============== DISCONTINUED ==========================\n",
    "# Workflow for Evaluating LLM Performance in a Text Classification Task using Text-Bison and Vertex AI SDK\n",
    "\n",
    "In this notebook, we will explore various aspects related to running the Vertex LLM evaluation pipeline. Our journey will encompass the following key stages:\n",
    "\n",
    "1. **Data Preparation**: Before we begin the evaluation process, we will ensure that our data is prepared and ready for input into the pipeline.\n",
    "\n",
    "2. **Evaluation with Model text-bison@001**: We will execute the evaluation phase using the foundational model, known as text-bison@001. This step is crucial for assessing the model's performance and establishing a baseline.\n",
    "\n",
    "3. **Metric Retrieval**: After completing the evaluation, we will extract valuable metrics generated as artifacts by the pipeline.\n",
    "\n",
    "4. **Metric Visualization**: In this notebook, we will present and visualize the collected metrics.\n",
    "\n",
    "5. **Tensorboard Upload and Visualization**: We will upload the metrics to Tensorboard. This platform will allow us to explore the metrics dynamically and interactively, enhancing our understanding.\n",
    "\n",
    "6. **Vertex Experiments**: In addition to Tensorboard, we will also explore another method for uploading and visualizing our metrics: the Vertex Experiments environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/notebook1.png\" style=\"width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Vertex AI LLM SDK (Private Preview)\n",
    "! pip install -U google-cloud-aiplatform\n",
    "! pip install \"shapely<2.0.0\"\n",
    "\n",
    "# Install HuggingFace Datasets\n",
    "! pip install datasets\n",
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL (if you are using Colab, restart the Kernel at this point, uncommend and execute the following code)\n",
    "# from google.colab import auth as google_auth\n",
    "# google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python packages and define project variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import vertexai\n",
    "import uuid\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from datasets import load_dataset\n",
    "from google.cloud import storage\n",
    "from sklearn import metrics\n",
    "from tabulate import tabulate\n",
    "from vertexai.preview.language_models import (\n",
    "    TextGenerationModel,\n",
    "    EvaluationTextClassificationSpec,\n",
    "    EvaluationTextGenerationSpec,\n",
    "    EvaluationQuestionAnsweringSpec,\n",
    "    EvaluationTextSummarizationSpec,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the values of the variables below according to your project specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project variables\n",
    "PROJECT_ID = \"<YOUR PROJECT ID>\"\n",
    "LOCATION = \"us-central1\"\n",
    "STAGING_BUCKET = \"gs://<YOUR BUCKET NAME>\"\n",
    "DATA_STAGING_GCS_LOCATION = \"gs://<YOUR BUCKET NAME>\"\n",
    "\n",
    "storage_client = storage.Client()\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Vertex AI TensorBoard instance\n",
    "\n",
    "Create an instance of Vertex AI Tensorboard that will be used to upload the evaluation metrics. \n",
    "\n",
    "If you want to reuse an existing instance, skip the following cell and set the `tensorboard_id` variable to your instance ID.  \n",
    "Note that the instance must be in the same region where the evaluation data was written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = 'llm-eval-tensorboard'\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "        display_name=display_name,\n",
    "        project=PROJECT_ID,\n",
    "        location=LOCATION\n",
    "    )\n",
    "\n",
    "print(tensorboard.display_name)\n",
    "print(tensorboard.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: projects/244831775715/locations/us-central1/tensorboards/1667462160080437248\n",
    "# Replace with the your Tensorboard resource name\n",
    "tensorboard_id = '<YOUR TENSORBOARD RESOURCE NAME>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset for evaluation\n",
    "\n",
    "In this lab, you are going to evaluate the **text-bison** foundation model for a single label text classification task. You are going to use the `dair-ai/emotion` dataset from HuggingFace.  \n",
    "Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from HuggingFace\n",
    "dataset = load_dataset('dair-ai/emotion', split='test[:5%]')\n",
    "print('Dataset structure:\\n', dataset)\n",
    "print('Sample:\\n', dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation dataset used for model evaluation includes **prompt** and **ground truth** pairs that align with the task that you want to evaluate. Your dataset must include a minimum of one prompt and ground truth pair, but we recommend at least 10 pairs for meaningful metrics. Generally speaking, the more examples you give, the more meaningful the results.\n",
    "\n",
    "The dataset can be in 2 different formats:\n",
    " - Pandas Dataframe\n",
    " - JSONL file on Google Cloud Storage\n",
    "\n",
    "Next we will demonstrate both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear',\n",
    "    5: 'surprise'\n",
    "}\n",
    "\n",
    "instructions = f'''Classify the text into one of the classes bellow: \n",
    "[{', '.join(class_labels.values())}]\n",
    "Text:\n",
    "'''\n",
    "\n",
    "def add_instructions(example, instructions):\n",
    "    example[\"prompt\"] = f'{instructions}{example[\"text\"]}'\n",
    "    example[\"ground_truth\"] = class_labels[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "eval_dataset = dataset.map(lambda x: add_instructions(x, instructions)).remove_columns(['text', 'label'])\n",
    "\n",
    "print(eval_dataset)\n",
    "print(eval_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataset split to GCS\n",
    "jsonl_filename = 'emotions-eval.jsonl'\n",
    "gcs_uri = f'{DATA_STAGING_GCS_LOCATION}/{jsonl_filename}'\n",
    "eval_dataset.to_json(jsonl_filename)\n",
    "\n",
    "# Copy file to GCS\n",
    "!gsutil cp {jsonl_filename} {gcs_uri}\n",
    "\n",
    "# List GCS bucket to verify the file was copied successfully\n",
    "!gsutil ls {DATA_STAGING_GCS_LOCATION}/*.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Vertex AI LLM Model Evaluation job\n",
    "\n",
    "As mentioned before, you can start an evaluation job passing a Pandas Dataframe or a path to a JSONL file on GCS. You will explore both possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1 - Run evaluation with JSONL on GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "\n",
    "task_spec_classification = EvaluationTextClassificationSpec(\n",
    "    ground_truth_data=[gcs_uri],\n",
    "    class_names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'],\n",
    "    target_column_name='ground_truth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.evaluate(task_spec=task_spec_classification)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - Run evaluation on a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pandas dataframe to submit your job\n",
    "task_spec_classification = EvaluationTextClassificationSpec(\n",
    "    ground_truth_data=pd.DataFrame(eval_dataset),\n",
    "    class_names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'],\n",
    "    target_column_name='ground_truth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.evaluate(task_spec=task_spec_classification)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all pipeline jobs with \"evaluation-llm-classification-pipeline\" that succeeded\n",
    "for name in aiplatform.PipelineJob.list(project=PROJECT_ID, filter=\"pipeline_name:*evaluation-llm-classification-pipeline*\"):\n",
    "    if name.state == 4: # SUCCEEDED\n",
    "        print(name.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_field_name='ground_truth'\n",
    "evaluation_class_labels=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise', 'UNKNOWN']\n",
    "\n",
    "experiment_name = 'notebook1-experiment-llm-custom'\n",
    "\n",
    "# Example: 'projects/244831775715/locations/us-central1/pipelineJobs/evaluation-llm-classification-pipeline-20230831205858'\n",
    "# Copy one of the resource names from the listing above\n",
    "pipeline_resource_name = '<YOUR PROJECT RESOURCE FULL NAME>'\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID, \n",
    "    location=LOCATION, \n",
    "    staging_bucket=STAGING_BUCKET, \n",
    "    experiment=experiment_name,\n",
    "    experiment_tensorboard=tensorboard_id)\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob.get(resource_name=pipeline_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1 - Local visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to read metrics content from GCS\n",
    "def get_metrics_blob(job):\n",
    "  expected_task_name = \"model-evaluation-classification\"\n",
    "  task_detail = None\n",
    "  for detail in job.task_details:\n",
    "    if detail.task_name == expected_task_name:\n",
    "      task_detail = detail\n",
    "  if not task_detail:\n",
    "    print(f\"Not able to find the task {expected_task_name}.\")\n",
    "  metrics_uri = None\n",
    "  for k, v in task_detail.outputs.items():\n",
    "    if k != \"evaluation_metrics\":\n",
    "      continue\n",
    "    for artifact in v.artifacts:\n",
    "      if artifact.display_name == \"evaluation_metrics\":\n",
    "        metrics_uri = artifact.uri[5:]\n",
    "  if not metrics_uri:\n",
    "    print(\"Not able to find the metric.\")\n",
    "  splits = metrics_uri.split(\"/\")\n",
    "  bucket_name = splits[0]\n",
    "  blob_name = '/'.join(splits[1:])\n",
    "  bucket = storage_client.bucket(bucket_name)\n",
    "  blob = bucket.blob(blob_name)\n",
    "  with blob.open(\"r\") as f:\n",
    "    return json.loads(f.read())\n",
    "  \n",
    "overall_metrics = get_metrics_blob(pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to print classification metrics\n",
    "def get_classification_metrics(overall_metrics):\n",
    "  classification_metrics = overall_metrics['slicedMetrics']\n",
    "  metric_names = [\"Metric Slice\", \"auPrc\", \"auRoc\", \"logLoss\"]\n",
    "  f1_metrics = [\"f1Score\"]\n",
    "  aggregated_f1_metrics = [\"f1ScoreMicro\", \"f1ScoreMacro\"]\n",
    "  table = [metric_names + f1_metrics + aggregated_f1_metrics]\n",
    "  for metrics in classification_metrics:\n",
    "    classification_metric = metrics['metrics']['classification']\n",
    "    slice_name = \"class - \" + metrics['singleOutputSlicingSpec']['value'] if 'value' in metrics['singleOutputSlicingSpec'] else \"Overall\"\n",
    "    slice_metric_values = [slice_name]\n",
    "    slice_metric_values.extend([classification_metric.get(metric_name, 0) for metric_name in metric_names[1:]])\n",
    "    slice_metric_values.extend([classification_metric['confidenceMetrics'][0].get(metric_name, 0) for metric_name in f1_metrics])\n",
    "    slice_metric_values.extend([classification_metric['confidenceMetrics'][0].get(metric_name, 'n/a') for metric_name in aggregated_f1_metrics])\n",
    "    table.append(slice_metric_values)\n",
    "  return table\n",
    "\n",
    "classification_metrics = get_classification_metrics(overall_metrics)\n",
    "print(tabulate(classification_metrics, headers='firstrow', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to plot confusion matrix\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "def get_confusion_matrix(overall_metrics):\n",
    "  confusion_matrix = []\n",
    "  for slice_metric in overall_metrics['slicedMetrics']:\n",
    "    if 'value' in slice_metric['singleOutputSlicingSpec']:\n",
    "      continue\n",
    "    if 'confusionMatrix' not in slice_metric['metrics']['classification']:\n",
    "      print(\"No Confusion Matrix found\")\n",
    "      print(f\"Evaluation metrics is: {slice_metric}\")\n",
    "      return\n",
    "    for row in slice_metric['metrics']['classification']['confusionMatrix']['rows']:\n",
    "      confusion_matrix.append(row['dataItemCounts'])\n",
    "  # Plot the matrix\n",
    "  return confusion_matrix\n",
    "\n",
    "confusion_matrix = get_confusion_matrix(overall_metrics)\n",
    "\n",
    "confusion_matrix_plot = numpy.array(confusion_matrix)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix_plot, display_labels = evaluation_class_labels)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "cm_display.plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to print confidence metrics\n",
    "def get_confidence_metrics(overall_metrics, expected_confidence_threshold):\n",
    "  all_metrics = overall_metrics['slicedMetrics']\n",
    "  confidence_metric_names = [\"Metric Slice\", \"recall\", \"precision\", \"falsePositiveRate\", \"f1Score\", \"truePositiveCount\", \"falsePositiveCount\"]\n",
    "  table = [confidence_metric_names]\n",
    "  for metrics in all_metrics:\n",
    "    classification_metric = metrics['metrics']['classification']\n",
    "    slice_name = \"class - \" + metrics['singleOutputSlicingSpec']['value'] if 'value' in metrics['singleOutputSlicingSpec'] else \"Overall\"\n",
    "    slice_metric_values = [slice_name]\n",
    "    confidence_metrics = None\n",
    "    found_threshold_distance = 1\n",
    "    for metrics in classification_metric['confidenceMetrics']:\n",
    "      confidence_threshold = metrics['confidenceThreshold'] if 'confidenceThreshold' in metrics else 0\n",
    "      if abs(expected_confidence_threshold-confidence_threshold) <= found_threshold_distance:\n",
    "        confidence_metrics = metrics\n",
    "        found_threshold_distance = abs(expected_confidence_threshold-confidence_threshold)\n",
    "    slice_metric_values.extend([confidence_metrics.get(metric_name, 0) for metric_name in confidence_metric_names[1:]])\n",
    "    table.append(slice_metric_values)\n",
    "  return table\n",
    "\n",
    "confidence_metrics = get_confidence_metrics(overall_metrics=overall_metrics, expected_confidence_threshold=0.9)\n",
    "print(tabulate(confidence_metrics, headers='firstrow', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 - Start ExperimentRun and log metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"run-{}\".format(uuid.uuid4())\n",
    "with aiplatform.start_run(run=run_name) as my_run:\n",
    "    metrics = {}\n",
    "    metrics['auPrc'] = classification_metrics[1][4]\n",
    "    metrics['auRoc'] = classification_metrics[1][5]\n",
    "    metrics['logLoss'] = classification_metrics[1][6]\n",
    "    metrics['f1Score'] = classification_metrics[1][4]\n",
    "    metrics['f1ScoreMicro'] = classification_metrics[1][5]\n",
    "    metrics['f1ScoreMacro'] = classification_metrics[1][6]\n",
    "    my_run.log_metrics(metrics)\n",
    "\n",
    "    aiplatform.log(pipeline_job=pipeline_job)\n",
    "\n",
    "    aiplatform.log_classification_metrics(\n",
    "        labels=evaluation_class_labels,\n",
    "        matrix=confusion_matrix,\n",
    "        display_name='confusion_matrix'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 3 - Log metrics to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "logdir = \"tf_logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with file_writer.as_default(step=0):\n",
    "    tf.summary.scalar(name='auPrc', data=classification_metrics[1][4])\n",
    "    tf.summary.scalar(name='auRoc', data=classification_metrics[1][5])\n",
    "    tf.summary.scalar(name='logLoss', data=classification_metrics[1][6])\n",
    "    tf.summary.scalar(name='f1Score', data=classification_metrics[1][4])\n",
    "    tf.summary.scalar(name='f1ScoreMicro', data=classification_metrics[1][5])\n",
    "    tf.summary.scalar(name='f1ScoreMacro', data=classification_metrics[1][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.upload_tb_log(\n",
    "    tensorboard_id=tensorboard_id,\n",
    "    tensorboard_experiment_name=experiment_name,\n",
    "    logdir=logdir\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
