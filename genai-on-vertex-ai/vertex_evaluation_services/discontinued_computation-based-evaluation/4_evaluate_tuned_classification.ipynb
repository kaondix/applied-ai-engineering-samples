{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | |\n",
    "|----------|-------------|\n",
    "| Author(s)   | Renato Leite (renatoleite@), Egon Soares (egon@) |\n",
    "| Last updated | 07/09/2024 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============== DISCONTINUED ==========================\n",
    "# Complete LLM Model Evaluation Workflow for Classification using KFP Pipelines\n",
    "\n",
    "In this notebook, we will explore various aspects related to running the Vertex LLM evaluation pipeline. Our journey will encompass the following key stages:\n",
    "\n",
    "1. **Data Preparation**: Before we begin the evaluation process, we'll ensure our data is prepared and ready for input into the pipeline.\n",
    "\n",
    "2. **Model Tuning**: We'll optimize the performance of the foundational model through tuning. We'll also monitor the tuning job's progress using a managed Tensorboard instance.\n",
    "\n",
    "3. **Evaluation with Tuned Model**: After tuning, we'll execute the evaluation phase using the tuned model. This step is critical for assessing the model's performance.\n",
    "\n",
    "4. **Baseline Evaluation with Model text-bison@001**: Additionally, we'll perform a baseline evaluation using the foundational model, text-bison@001. This will provide a benchmark for model performance assessment.\n",
    "\n",
    "5. **Metric Analysis**: Following the evaluations, we'll visualize all the metrics within the Vertex AI Model Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/notebook4.png\" style=\"width:1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Vertex AI LLM SDK (Private Preview)\n",
    "! pip install -U google-cloud-aiplatform\n",
    "! pip install -U google-cloud-pipeline-components\n",
    "! pip install \"shapely<2.0.0\"\n",
    "\n",
    "# Install HuggingFace Datasets\n",
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL (if you are using Colab, restart the Kernel at this point, uncommend and execute the following code)\n",
    "# from google.colab import auth as google_auth\n",
    "# google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python packages and define project variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import vertexai\n",
    "import uuid\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google_cloud_pipeline_components.preview.model_evaluation import evaluation_llm_classification_pipeline\n",
    "from kfp import compiler\n",
    "from kfp import dsl\n",
    "\n",
    "from vertexai.preview.language_models import (\n",
    "    TextGenerationModel,\n",
    "    EvaluationTextClassificationSpec,\n",
    "    TuningEvaluationSpec\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the values of the variables below according to your project specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project variables\n",
    "PROJECT_ID = \"rl-llm-dev\"\n",
    "\n",
    "ENDPOINT_LOCATION = \"us-central1\"\n",
    "STAGING_BUCKET = \"gs://<YOUR BUCKET NAME>\"    # Same location as ENDPOINT_LOCATION\n",
    "\n",
    "TUNING_JOB_LOCATION = \"us-central1\"\n",
    "DATA_STAGING_GCS_LOCATION = \"gs://<YOUR BUCKET NAME>\"    # Same location as ENDPOINT_LOCATION\n",
    "\n",
    "storage_client = storage.Client()\n",
    "vertexai.init(project=PROJECT_ID, location=ENDPOINT_LOCATION, staging_bucket=STAGING_BUCKET)\n",
    "aiplatform.init(project=PROJECT_ID, location=ENDPOINT_LOCATION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Vertex AI TensorBoard instance\n",
    "\n",
    "The Adapter Tuning pipeline can log the training metrics for tracking and retrospective analysis. \n",
    "\n",
    "Create an instance of Vertex AI Tensorboard that will be used by tuning pipeline runs. \n",
    "\n",
    "If you want to reuse an existing instance, skip the following cell and set the `tensorboard_id` variable to your instance ID. Note that the instance must be in the same region where the tuning jobs will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = 'notebook4-llm-eval-tensorboard'\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "        display_name=display_name,\n",
    "        project=PROJECT_ID,\n",
    "        location=TUNING_JOB_LOCATION,\n",
    "    )\n",
    "\n",
    "print(tensorboard.display_name)\n",
    "print(tensorboard.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your Tensorboard ID\n",
    "# Example: tensorboard_id = '6279148178507825152'\n",
    "tensorboard_id = '<YOUR TENSORBOARD ID>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training dataset\n",
    "\n",
    "In this lab, you are going to tune the **text-bison** foundation model for a single label text classification task. You are going to use the `dair-ai/emotion` dataset from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('dair-ai/emotion')\n",
    "print(dataset)\n",
    "print(dataset['test'][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {k:v for (k,v) in zip(['train', 'validation', 'test'],\n",
    "                                 load_dataset('dair-ai/emotion', split=['train[0:7200]', 'validation[0:256]', 'test[0:256]']))}\n",
    "dataset = DatasetDict(splits)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to the format required by the tuning pipeline\n",
    "\n",
    "Your model tuning dataset must be in JSON Lines (JSONL) format where each line contains a single tuning example. Each example is composed of an `input_text` field that contains the prompt to the model and an `output_text` field that contains an example response that the tuned model is expected to produce. The maximum token length for input_text is 8,192 and the maximum token length for output_text is 1,024. If either fields exceed the maximum token length, the excess tokens are truncated.\n",
    "\n",
    "The examples included in your dataset should match your expected production traffic. If your dataset contains specific formatting, keywords, instructions, or information, the production data should be formatted in the same way and contain the same instructions.\n",
    "\n",
    "For example, if the examples in your dataset include a `\"question:\"` and a `\"context:\"`, production traffic should also be formatted to include a `\"question:\"` and a `\"context:\"` in the same order as it appears in the dataset examples. If you exclude the context, the model will not recognize the pattern, even if the exact question was in an example in the dataset.\n",
    "\n",
    "For tasks such as classification, it is possible to create a dataset of examples that don't contain instructions. However, excluding instructions from the examples in the dataset leads to worse performance after tuning than including instructions, especially for smaller datasets.\n",
    "\n",
    "For our dataset, we are going to add the following instructions\n",
    "\n",
    "```\n",
    "Classify the following as one of the following categories:\n",
    "- sadness,\n",
    "- joy,\n",
    "Text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear',\n",
    "    5: 'surprise'\n",
    "}\n",
    "\n",
    "class_labels.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = f'''Classify the following text into one of the following classes: \n",
    "[{', '.join(class_labels.values())}]\n",
    "Text:\n",
    "'''\n",
    "\n",
    "def add_instructions(example, instructions):\n",
    "    example[\"input_text\"] = f'{instructions}{example[\"text\"]}'\n",
    "    example[\"output_text\"] = class_labels[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "tuning_dataset = dataset.map(lambda x: add_instructions(x, instructions)).remove_columns(['text', 'label'])\n",
    "\n",
    "print(tuning_dataset)\n",
    "print(tuning_dataset['train'][:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the dataset splits to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_uris = {}\n",
    "filename_prefix = 'emotion'\n",
    "\n",
    "for split_name, split_data in tuning_dataset.items():\n",
    "    jsonl_filename = f'{filename_prefix}-{split_name}.jsonl'\n",
    "    gcs_uri = f'{DATA_STAGING_GCS_LOCATION}/{jsonl_filename}'\n",
    "    gcs_uris[split_name] = gcs_uri\n",
    "    split_data.to_json(jsonl_filename)\n",
    "    !gsutil cp {jsonl_filename} {gcs_uri}\n",
    "\n",
    "!gsutil ls {DATA_STAGING_GCS_LOCATION}/*.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the evaluation dataset split to GCS\n",
    "jsonl_filename = 'emotions-eval.jsonl'\n",
    "evaluation_dataset_gcs_uri = f'{STAGING_BUCKET}/{jsonl_filename}'\n",
    "evaluation_dataset = tuning_dataset['test'].rename_column('input_text', 'prompt').rename_column('output_text', 'ground_truth')\n",
    "evaluation_dataset.to_json(jsonl_filename)\n",
    "\n",
    "# Copy file to GCS\n",
    "!gsutil cp {jsonl_filename} {evaluation_tuned_gcs_uri}\n",
    "\n",
    "# List GCS bucket to verify the file was copied successfully\n",
    "!gsutil ls {STAGING_BUCKET}/*.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning and Evaluation Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.preview.model_evaluation import evaluation_llm_classification_pipeline\n",
    "from google.cloud.aiplatform import PipelineJob\n",
    "\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from kfp import dsl, components\n",
    "from kfp.dsl import Input, Output, Markdown, Artifact\n",
    "\n",
    "tune_large_model = components.load_component_from_url(\n",
    "    'https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0')\n",
    "\n",
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'google_cloud_pipeline_components',  \n",
    "        'google-cloud-storage',\n",
    "        'pandas']\n",
    ")\n",
    "def record_metrics_component(\n",
    "    evaluation_class_labels: list,\n",
    "    evaluation_metrics: Input[artifact_types.ClassificationMetrics],\n",
    "    confusion_artifact: Output[dsl.ClassificationMetrics],\n",
    "    classification_artifact: Output[Markdown],\n",
    "    raw_metrics: Output[dsl.Metrics]\n",
    "):\n",
    "    import json\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Read metrics content from GCS\n",
    "    def get_metrics_blob(metrics_uri):\n",
    "        splits = metrics_uri.split(\"/\")\n",
    "        bucket_name = splits[2]\n",
    "        blob_name = '/'.join(splits[3:])\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        with blob.open(\"r\") as f:\n",
    "            return json.loads(f.read())\n",
    "\n",
    "    def get_confusion_matrix(overall_metrics):\n",
    "        confusion_matrix = []\n",
    "        for slice_metric in overall_metrics['slicedMetrics']:\n",
    "            if 'value' in slice_metric['singleOutputSlicingSpec']:\n",
    "                continue\n",
    "            for row in slice_metric['metrics']['classification']['confusionMatrix']['rows']:\n",
    "                confusion_matrix.append(row['dataItemCounts'])\n",
    "        return confusion_matrix\n",
    "\n",
    "    # Define the function to print classification metrics\n",
    "    def get_classification_metrics(overall_metrics):\n",
    "        all_metrics = overall_metrics['slicedMetrics']\n",
    "        metric_names = [\"Metric Slice\", \"auPrc\", \"auRoc\", \"logLoss\"]\n",
    "        f1_metrics = [\"f1Score\"]\n",
    "        aggregated_f1_metrics = [\"f1ScoreMicro\", \"f1ScoreMacro\"]\n",
    "        table = [metric_names + f1_metrics + aggregated_f1_metrics]\n",
    "        for metrics in all_metrics:\n",
    "            classification_metric = metrics['metrics']['classification']\n",
    "            slice_name = \"class - \" + metrics['singleOutputSlicingSpec']['value'] if 'value' in metrics['singleOutputSlicingSpec'] else \"Overall\"\n",
    "            slice_metric_values = [slice_name]\n",
    "            slice_metric_values.extend(\n",
    "                [classification_metric.get(metric_name, 0) \n",
    "                 for metric_name in metric_names[1:]])\n",
    "            slice_metric_values.extend(\n",
    "                [classification_metric['confidenceMetrics'][0].get(metric_name, 0) \n",
    "                 for metric_name in f1_metrics])\n",
    "            slice_metric_values.extend(\n",
    "                [classification_metric['confidenceMetrics'][0].get(metric_name, 'n/a') \n",
    "                 for metric_name in aggregated_f1_metrics])\n",
    "            table.append(slice_metric_values)\n",
    "        return table\n",
    "\n",
    "    # Log Confusion Matrix artifact\n",
    "    overall_metrics = get_metrics_blob(metrics_uri=evaluation_metrics.uri)\n",
    "    confusion_matrix = get_confusion_matrix(overall_metrics)\n",
    "    evaluation_class_labels.append('UNKNOWN')\n",
    "    confusion_artifact.log_confusion_matrix(\n",
    "        categories=evaluation_class_labels,\n",
    "        matrix=confusion_matrix\n",
    "    )\n",
    "\n",
    "    # Log Classification metrics\n",
    "    metrics_table = get_classification_metrics(overall_metrics)\n",
    "    markdown_content = pd.DataFrame(metrics_table).to_markdown()\n",
    "    with open(classification_artifact.path, 'w') as fp:\n",
    "        fp.write(markdown_content)\n",
    "\n",
    "    # Log Raw metrics\n",
    "    raw_metrics.log_metric(\n",
    "        metric='f1Score',\n",
    "        value=metrics_table[1][4]\n",
    "    )\n",
    "    \n",
    "    # Log Raw metrics\n",
    "    raw_metrics.log_metric(\n",
    "        metric='f1ScoreMicro',\n",
    "        value=metrics_table[1][5]\n",
    "    )\n",
    "    \n",
    "    # Log Raw metrics\n",
    "    raw_metrics.log_metric(\n",
    "        metric='f1ScoreMacro',\n",
    "        value=metrics_table[1][6]\n",
    "    )\n",
    "\n",
    "@dsl.pipeline\n",
    "def complete_evaluation_pipeline(\n",
    "    project: str,\n",
    "    training_dataset_uri: str,\n",
    "    evaluation_data_uri: str,\n",
    "    tensorboard_id: str,\n",
    "    evaluation_class_labels: list,\n",
    "    evaluation_tuned_output_uri: str,\n",
    "    evaluation_tuned_input_uris: list,\n",
    "    evaluation_bison_output_uri: str,\n",
    "    evaluation_bison_input_uris: list,\n",
    "    bison_model_name: str\n",
    "):\n",
    "    # tune com tensorboard + evaluation no tuned model\n",
    "    model_resources = tune_large_model(\n",
    "        model_display_name='notebook4-tuned-model',\n",
    "        location='us-central1',\n",
    "        large_model_reference='text-bison@001',\n",
    "        project=project,\n",
    "        train_steps=2,\n",
    "        dataset_uri=training_dataset_uri,\n",
    "        evaluation_interval=1,\n",
    "        evaluation_data_uri=evaluation_data_uri,\n",
    "        tensorboard_resource_id=tensorboard_id\n",
    "    ).set_display_name(name='Tune foundational model')\n",
    "\n",
    "    tuned_model_evaluation = evaluation_llm_classification_pipeline(\n",
    "        project=project,\n",
    "        location='us-central1',\n",
    "        batch_predict_gcs_destination_output_uri=evaluation_tuned_output_uri,\n",
    "        evaluation_class_labels=evaluation_class_labels,\n",
    "        batch_predict_gcs_source_uris=evaluation_tuned_input_uris,\n",
    "        target_field_name='ground_truth',\n",
    "        model_name=model_resources.outputs['model_resource_name']\n",
    "    ).set_display_name(name='Evaluate tuned model')\n",
    "\n",
    "    record_metrics_component(\n",
    "        evaluation_class_labels=evaluation_class_labels,\n",
    "        evaluation_metrics=tuned_model_evaluation.outputs[\n",
    "            'evaluation_metrics']).set_display_name(name=\"Record tuned model evaluation metrics\")\n",
    "\n",
    "    eval_pipeline = evaluation_llm_classification_pipeline(\n",
    "        project=project,\n",
    "        location='us-central1',\n",
    "        batch_predict_gcs_destination_output_uri=evaluation_bison_output_uri,\n",
    "        evaluation_class_labels=evaluation_class_labels,\n",
    "        batch_predict_gcs_source_uris=evaluation_bison_input_uris,\n",
    "        target_field_name='ground_truth',\n",
    "        model_name=bison_model_name\n",
    "    ).set_display_name(name=\"Evaluate foundational model\")\n",
    "\n",
    "    record_metrics_component(\n",
    "        evaluation_class_labels=evaluation_class_labels,\n",
    "        evaluation_metrics=eval_pipeline.outputs[\n",
    "            'evaluation_metrics']).set_display_name(name=\"Record foundational model evaluation metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = TextGenerationModel.from_pretrained('text-bison@001')\n",
    "model_name = base_model._model_resource_name\n",
    "\n",
    "job_id = \"custom-model-evaluation-{}\".format(uuid.uuid4())\n",
    "experiment_name = 'notebook4-complete-classification-pipeline'\n",
    "\n",
    "target_field_name='ground_truth'\n",
    "tuned_class_labels=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID, \n",
    "    location=ENDPOINT_LOCATION, \n",
    "    staging_bucket=STAGING_BUCKET,\n",
    "    experiment=experiment_name,\n",
    "    experiment_tensorboard=tensorboard_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_classification_pipeline_path = 'complete_classification_pipeline_path.json'\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=complete_evaluation_pipeline,\n",
    "    package_path=complete_classification_pipeline_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"project\": PROJECT_ID,\n",
    "    \"evaluation_class_labels\": tuned_class_labels,\n",
    "    \"evaluation_tuned_output_uri\": f'{STAGING_BUCKET}/output',\n",
    "    \"evaluation_tuned_input_uris\": [evaluation_dataset_gcs_uri],\n",
    "    \"training_dataset_uri\": gcs_uris['train'],\n",
    "    \"evaluation_data_uri\": gcs_uris['validation'],\n",
    "    \"tensorboard_id\": tensorboard_id,\n",
    "    \"evaluation_bison_output_uri\": f'{STAGING_BUCKET}/output',\n",
    "    \"evaluation_bison_input_uris\": [evaluation_dataset_gcs_uri],\n",
    "    \"bison_model_name\": model_name,\n",
    "}\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    template_path=complete_classification_pipeline_path,\n",
    "    pipeline_root=STAGING_BUCKET,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=True,\n",
    "    location='us-central1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.submit(experiment=experiment_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
