{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "CBk3jQ3fVWUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ingestion of Unstructured Documents with Metadata in Vertex AI Search\n",

        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-ba_extbygoog_notebook-from_notebook-colab&utm_medium=aRT-clicks&utm_campaign=ba_extbygoog_notebook-from_notebook-colab&destination=ba_extbygoog_notebook-from_notebook-colab&url=https%3A%2F%2Fcolab.research.google.com%2Fgithub%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fblob%2Fmain%2Fgenai-on-vertex-ai%2Fvertex_ai_search%2Fingesting_unstructured_docuemtns_with_metadata.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-ba_extbygoog_notebook-from_notebook-colab_ent&utm_medium=aRT-clicks&utm_campaign=ba_extbygoog_notebook-from_notebook-colab_ent&destination=ba_extbygoog_notebook-from_notebook-colab_ent&url=https%3A%2F%2Fconsole.cloud.google.com%2Fvertex-ai%2Fcolab%2Fimport%2Fhttps%3A%252F%252Fraw.githubusercontent.com%252FGoogleCloudPlatform%252Fapplied-ai-engineering-samples%252Fmain%252Fgenai-on-vertex-ai%252Fvertex_ai_search%2Fingesting_unstructured_docuemtns_with_metadata.ipynb\"\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-ba_extbygoog_notebook-from_notebook-vai_workbench&utm_medium=aRT-clicks&utm_campaign=ba_extbygoog_notebook-from_notebook-vai_workbench&destination=ba_extbygoog_notebook-from_notebook-vai_workbench&url=https%3A%2F%2Fconsole.cloud.google.com%2Fvertex-ai%2Fworkbench%2Fdeploy-notebook%3Fdownload_url%3Dhttps%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fmain%2Fgenai-on-vertex-ai%2Fvertex_ai_search%2Fingesting_unstructured_docuemtns_with_metadata.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-ba_extbygoog_notebook-from_notebook-github&utm_medium=aRT-clicks&utm_campaign=ba_extbygoog_notebook-from_notebook-github&destination=ba_extbygoog_notebook-from_notebook-github&url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fblob%2Fmain%2Fgenai-on-vertex-ai%2Fvertex_ai_search%2Fingesting_unstructured_docuemtns_with_metadata.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "Z6z9Ibm0VXRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| | |\n",
        "|----------|-------------|\n",
        "| Author(s)   | Hossein Mansour|\n",
        "| Reviewers(s) | Meltem Subasioglu|\n",
        "| Last updated | 2024-07-18: The first draft |"
      ],
      "metadata": {
        "id": "rhWHRiVePfQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "In this notebook, we will show you how to prepare and ingest unstructured documents with metadata into [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/introduction). Metadata can be used for different purposes such as improving recall and precision, influencing results via boosting and filtering, and including additional context to be retrieved together with the documents. You can find more information about different types of metadata [here](https://cloud.google.com/generative-ai-app-builder/docs/provide-schema#about_providing_your_own_schema_as_a_json_object).\n",
        "\n",
        "We will perform the following steps:\n",
        "\n",
        "- Creating a Vertex AI Search Datastore\n",
        "- Creating a Vertex AI Search App\n",
        "- [Optional] Updating the Schema for the Datastore\n",
        "- Reading Documents and their Metadata from a GCS bucket and combining them together as JSONL file\n",
        "- Uploading the documents with their metadata to the Datastore\n",
        "- Searching the Datastore\n",
        "\n",
        "\n",
        "Please refer to the [official documentation](https://cloud.google.com/generative-ai-app-builder/docs/create-datastore-ingest) for the definition of Datastores and Apps and their relationships to one another\n",
        "\n",
        "REST API is used throughout this notebook. Please consult the [official documentation](https://cloud.google.com/generative-ai-app-builder/docs/apis) for alternative ways to achieve the same goal, namely Client libraries and RPC.\n",
        "\n",
        "\n",
        "# Vertex AI Search\n",
        "Vertex AI Search (VAIS) is a fully-managed platform, powered by large language models, that lets you build AI-enabled search and recommendation experiences for your public or private websites or mobile applications\n",
        "\n",
        "VAIS can handle a diverse set of data sources including structured, unstructured, and website data, as well as data from third-party applications such as Jira, Salesforce, and Confluence.\n",
        "\n",
        "VAIS also has built-in integration with LLMs which enables you to provide answers to complex questions, grounded in your data\n",
        "\n",
        "#Using this Notebook\n",
        "If you're running outside of Colab, depending on your environment you may need to install pip packages that are included in the Colab environment by default but are not part of the Python Standard Library. Outside of Colab you'll also notice comments in code cells that look like #@something, these trigger special Colab functionality but don't change the behavior of the notebook.\n",
        "\n",
        "This tutorial uses the following Google Cloud services and resources:\n",
        "\n",
        "- Service Usage API\n",
        "- Discovery Engine\n",
        "- Google Cloud Storage Client\n",
        "\n",
        "This notebook has been tested in the following environment:\n",
        "\n",
        "- Python version = 3.10.12\n",
        "- google.cloud.storage = 2.8.0\n",
        "- google.auth = 2.27.0\n",
        "\n",
        "# Getting Started\n",
        "\n",
        "The following steps are necessary to run this notebook, no matter what notebook environment you're using.\n",
        "\n",
        "If you're entirely new to Google Cloud, [get started here](https://cloud.google.com/docs/get-started)\n",
        "\n",
        "## Google Cloud Project Setup\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
        "3. [Enable the Service Usage API](https://console.cloud.google.com/apis/library/serviceusage.googleapis.com)\n",
        "4. [Enable the Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com)\n",
        "5. [Enable the Discovery Engine API for your project](https://console.cloud.google.com/marketplace/product/google/discoveryengine.googleapis.com)\n",
        "\n",
        "## Google Cloud Permissions\n",
        "\n",
        "Ideally you should have [Owner role](https://cloud.google.com/iam/docs/understanding-roles) for your project to run this notebook. If that is not an option, you need at least the following [roles](https://cloud.google.com/iam/docs/granting-changing-revoking-access)\n",
        "- **`roles/serviceusage.serviceUsageAdmin`** to enable APIs\n",
        "- **`roles/iam.serviceAccountAdmin`** to modify service agent permissions\n",
        "- **`roles/discoveryengine.admin`** to modify discoveryengine assets\n",
        "- **`roles/storage.objectAdmin`** to modify and delete GCS buckets\n"
      ],
      "metadata": {
        "id": "GVFVUibCCiAD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "_OyCUmMVGeo-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "gCIgR1NCatrP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import re\n",
        "from typing import Dict, Any\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "import google.auth.transport.requests\n",
        "from google.cloud import storage\n",
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authentication"
      ],
      "metadata": {
        "id": "YqcV8aj8GvZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "creds, _ = default()\n",
        "auth_req = google.auth.transport.requests.Request()\n",
        "creds.refresh(auth_req)\n",
        "REST_TOKEN = 'Bearer ' + creds.token # for the ease of making REST calls"
      ],
      "metadata": {
        "id": "kT3Eda7_mlTP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the following constants to reflect your environment\n",
        "\n",
        "You can enter the ID for an existing App and Datastore to be used in this notebook.\n",
        "\n",
        "Alternatively, you can enter the desired IDs for non-existings App and Datastore and they will be created later in this notebook.\n",
        "\n",
        "Same applies to the GCS Directory of Documents and Metadata\n",
        "\n",
        "You can find more information regarding the \"Location\" of datastores and associated limitations [here](https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store).\n",
        "\n",
        "The Location of a Datastore is set at the time of creation and it should be called appropriately to query the Datastore."
      ],
      "metadata": {
        "id": "KTSL1m_CHFBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Vertex AI Search Parameters\n",
        "DATASTORE_ID = \"\"  # @param {type:\"string\"}\n",
        "APP_ID = \"\"  # @param {type:\"string\"}\n",
        "LOCATION = \"global\"  # @param [\"global\", \"us\", \"eu\"] Global is preferred\n",
        "\n",
        "# GCS Parameters, e.g. 'gs://my_bucket/folder1/docs/'\n",
        "GCS_DIRECTORY_DOCS = ''  # @param {type:\"string\"}\n",
        "GCS_DIRECTORY_METADATA = ''  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "089_6PdMa64e"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Pre requisite] Create a GCS bucket with sample documents\n",
        "\n",
        "This step is only needed for the purpose of this demo.\n",
        "\n",
        "For the real use case you will need to upload your actual docuemnts to a GCS bucket\n",
        "\n",
        "Here, we download Alphabet's Q1-Q4 Earning transcripts as sample documents"
      ],
      "metadata": {
        "id": "2sSInI07MCll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gcs_bucket_and_download_files(project_id, new_bucket_path, file_urls):\n",
        "    \"\"\"\n",
        "    Creates a new GCS bucket (if it doesn't exist) and downloads files from specified URLs.\n",
        "\n",
        "    Handles paths with subdirectories correctly using `urlparse`.\n",
        "    \"\"\"\n",
        "\n",
        "    storage_client = storage.Client(project=project_id)\n",
        "\n",
        "    # Extract bucket name and prefix from path\n",
        "    parsed_path = urlparse(new_bucket_path)\n",
        "    new_bucket_name = parsed_path.netloc\n",
        "    blob_prefix = parsed_path.path.strip('/')  # Remove leading and trailing slashes\n",
        "\n",
        "    new_bucket = storage_client.bucket(new_bucket_name)\n",
        "\n",
        "    if not new_bucket.exists():\n",
        "        new_bucket = storage_client.create_bucket(new_bucket_name)\n",
        "        print(f\"Bucket {new_bucket_name} created.\")\n",
        "\n",
        "    for url in file_urls:\n",
        "        file_name = url.split(\"/\")[-1]\n",
        "        print(f\"Downloading: {file_name}\")\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Construct the full blob path (including prefix)\n",
        "            blob_name = f\"{blob_prefix}/{file_name}\" if blob_prefix else file_name\n",
        "            blob = new_bucket.blob(blob_name)\n",
        "\n",
        "            blob.upload_from_string(response.content)\n",
        "            print(f\"Uploaded: {blob_name}\")  # Print the uploaded blob path\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error downloading {file_name}: {e}\")\n",
        "\n",
        "\n",
        "file_urls = [\n",
        "    \"https://abc.xyz/assets/investor/static/pdf/2022_Q1_Earnings_Transcript.pdf\",\n",
        "    \"https://abc.xyz/assets/investor/static/pdf/2022_Q2_Earnings_Transcript.pdf\",\n",
        "    \"https://abc.xyz/assets/investor/static/pdf/2022_Q3_Earnings_Transcript.pdf\",\n",
        "    \"https://abc.xyz/assets/investor/static/pdf/2022_Q4_Earnings_Transcript.pdf\"\n",
        "]\n",
        "\n",
        "create_gcs_bucket_and_download_files(PROJECT_ID, GCS_DIRECTORY_DOCS, file_urls)"
      ],
      "metadata": {
        "id": "Yg0Y_T_iLx_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Pre requisite] Create a GCS bucket with sample Metadata\n",
        "\n",
        "Similar to the code block above, this step is only needed for the purpose of this demo.\n",
        "\n",
        "Here we extract some trivial metadata from the file name. Each Metadata will have a content similar to the one below:\n",
        "\n",
        "```json\n",
        " {\n",
        "     \"doc_name\": \"2022_Q1_Earnings_Transcript\",\n",
        "     \"year\": \"2022\",\n",
        "     \"quarter\": \"Q1\",\n",
        "     \"doc_type\": \"earnings transcript\",\n",
        "     \"stock_tickers\": [\"GOOG\", \"GOOGL\"],\n",
        "     \"company_name\": \"alphabet\",\n",
        " }\n",
        " ```"
      ],
      "metadata": {
        "id": "8qFNMjxJMuEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_metadata_files(source_folder_path, metadata_folder_path):\n",
        "    \"\"\"Creates metadata JSON files for documents in a GCS folder.\"\"\"\n",
        "\n",
        "    bucket_name = source_folder_path.split(\"/\")[2]\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    source_folder = source_folder_path.replace(f\"gs://{bucket_name}/\", \"\")\n",
        "    metadata_folder = metadata_folder_path.replace(f\"gs://{bucket_name}/\", \"\")\n",
        "\n",
        "    blobs = bucket.list_blobs(prefix=source_folder)\n",
        "\n",
        "    for blob in blobs:\n",
        "        # Explicitly check if the blob is a folder/directory\n",
        "        if blob.name.endswith(\"/\"):\n",
        "            print(f\"Skipping folder: {blob.name}\")\n",
        "            continue\n",
        "\n",
        "        # Get the filename by splitting on the last \"/\"\n",
        "        filename = blob.name.split(\"/\")[-1]\n",
        "\n",
        "        # Improved regex to match a wider variety of file names\n",
        "        doc_name_match = re.match(r\"(\\d{4})_Q(\\d)_\\w+_Transcript\\.pdf\", filename)\n",
        "        if not doc_name_match:\n",
        "            print(f\"Skipping file with unexpected name: {filename}\")\n",
        "            continue\n",
        "\n",
        "        year, quarter = doc_name_match.groups()\n",
        "\n",
        "        # Construct doc_type from the filename (without path)\n",
        "        doc_type = \"_\".join(filename.split(\"_\")[2:-1]).replace(\"_\", \" \")\n",
        "\n",
        "        metadata = {\n",
        "            \"doc_name\": filename.replace(\".pdf\", \"\"),\n",
        "            \"year\": year,\n",
        "            \"quarter\": f\"Q{quarter}\",\n",
        "            \"doc_type\": doc_type,\n",
        "            \"stock_tickers\": [\"GOOG\", \"GOOGL\"],\n",
        "            \"company_name\": \"alphabet\"\n",
        "        }\n",
        "\n",
        "        metadata_file_name = f\"{metadata['doc_name']}.txt\"\n",
        "        metadata_blob = bucket.blob(metadata_folder + metadata_file_name)\n",
        "\n",
        "        metadata_blob.upload_from_string(json.dumps(metadata, indent=4))\n",
        "\n",
        "        print(f\"Created metadata file: {metadata_blob.name}\")\n",
        "\n",
        "\n",
        "create_metadata_files(GCS_DIRECTORY_DOCS, GCS_DIRECTORY_METADATA)"
      ],
      "metadata": {
        "id": "xh_kZyI1MkpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions to issue basic search on a Datastore or an App"
      ],
      "metadata": {
        "id": "C2hXlewDINDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_by_datastore(project_id: str, location: str, datastore_id: str, query: str) -> requests.Response:\n",
        "    \"\"\"Searches a datastore using the provided query.\"\"\"\n",
        "    response = requests.post(\n",
        "        f'https://discoveryengine.googleapis.com/v1/projects/{project_id}/locations/{location}/collections/default_collection/dataStores/{datastore_id}/servingConfigs/default_search:search',\n",
        "        headers={\n",
        "            'Content-Type': 'application/json',\n",
        "            'Authorization': REST_TOKEN,\n",
        "        },\n",
        "        json={\n",
        "            \"query\": query,\n",
        "            \"pageSize\": 1\n",
        "        },\n",
        "    )\n",
        "    return response\n",
        "\n",
        "\n",
        "def search_by_app(project_id: str, location: str, app_id: str, query: str) -> requests.Response:\n",
        "    \"\"\"Searches an app using the provided query.\"\"\"\n",
        "    response = requests.post(\n",
        "        f'https://discoveryengine.googleapis.com/v1/projects/{project_id}/locations/{location}/collections/default_collection/engines/{app_id}/servingConfigs/default_config:search',\n",
        "        headers={\n",
        "            'Content-Type': 'application/json',\n",
        "            'Authorization': REST_TOKEN,\n",
        "        },\n",
        "        json={\n",
        "            \"query\": query,\n",
        "            \"pageSize\": 1\n",
        "        },\n",
        "    )\n",
        "    return response"
      ],
      "metadata": {
        "id": "v-XHQIOooshe"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper fucntions to check whether or not a Datastore or an App already exist"
      ],
      "metadata": {
        "id": "eAigF6KHkMZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def datastore_exists(project_id: str, location: str, datastore_id: str) -> bool:\n",
        "    \"\"\"Check if a datastore exists.\"\"\"\n",
        "    response = search_by_datastore(project_id, location, datastore_id, \"test\")\n",
        "    status_code = response.status_code\n",
        "    if status_code == 200:\n",
        "        return True\n",
        "    if status_code == 404:\n",
        "        return False\n",
        "    raise Exception(f\"Error: {status_code}\")\n",
        "\n",
        "def app_exists(project_id: str, location: str, app_id: str) -> bool:\n",
        "    \"\"\"Check if an App exists.\"\"\"\n",
        "    response = search_by_app(project_id, location, app_id, \"test\")\n",
        "    status_code = response.status_code\n",
        "    if status_code == 200:\n",
        "        return True\n",
        "    if status_code == 404:\n",
        "        return False\n",
        "    raise Exception(f\"Error: {status_code}\")"
      ],
      "metadata": {
        "id": "IO1AxLZckXYK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper fucntions to create a Datastore or an App\n",
        "\n",
        "The datastore is created with [Chunk Mode](https://cloud.google.com/generative-ai-app-builder/docs/parse-chunk-documents) and Chunk size of 500 tokens.\n",
        "\n",
        "The documents will be processed with Layout parser and Ancestor information is included with each Chunk.\n",
        "\n",
        "These settings are chosen to optimize accuracy, they can be adjusted in the create_datastore function above."
      ],
      "metadata": {
        "id": "dsUql1_wkeaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_datastore(project_id: str, location: str, datastore_id: str) -> int:\n",
        "    \"\"\"Create a datastore.\"\"\"\n",
        "    payload = {\n",
        "        \"displayName\": datastore_id,\n",
        "        \"industryVertical\": \"GENERIC\",\n",
        "        \"solutionTypes\": [\"SOLUTION_TYPE_SEARCH\"],\n",
        "        \"contentConfig\": \"CONTENT_REQUIRED\",\n",
        "        \"documentProcessingConfig\": {\n",
        "            \"chunkingConfig\": {\n",
        "                \"layoutBasedChunkingConfig\": {\n",
        "                    \"chunkSize\": 500,\n",
        "                    \"includeAncestorHeadings\": True,\n",
        "                }\n",
        "            },\n",
        "            \"defaultParsingConfig\": {\n",
        "                \"layoutParsingConfig\": {}\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    header = {\"Authorization\": REST_TOKEN, \"X-Goog-User-Project\": project_id, \"Content-Type\": \"application/json\"}\n",
        "    es_endpoint = f\"https://discoveryengine.googleapis.com/v1/projects/{project_id}/locations/{location}/collections/default_collection/dataStores?dataStoreId={datastore_id}\"\n",
        "    response = requests.post(es_endpoint, data=json.dumps(payload), headers=header)\n",
        "    if response.status_code == 200:\n",
        "        print(f\"The creation of Datastore {datastore_id} is initiated.\")\n",
        "        print(\"It may take a few minutes for the Datastore to become available\")\n",
        "    else:\n",
        "        print(f\"Failed to create Datastore {datastore_id}\")\n",
        "        print(response.json())\n",
        "    return response.status_code\n",
        "\n",
        "def create_app(project_id: str, location: str, datastore_id: str, app_id: str) -> int:\n",
        "    \"\"\"Create a search app.\"\"\"\n",
        "    payload = {\n",
        "        \"displayName\": app_id,\n",
        "        \"dataStoreIds\": [datastore_id],\n",
        "        \"solutionType\": \"SOLUTION_TYPE_SEARCH\",\n",
        "        \"searchEngineConfig\": {\n",
        "            \"searchTier\": \"SEARCH_TIER_ENTERPRISE\",\n",
        "            \"searchAddOns\": [\"SEARCH_ADD_ON_LLM\"],\n",
        "        }\n",
        "    }\n",
        "    header = {\"Authorization\": REST_TOKEN, \"X-Goog-User-Project\": project_id, \"Content-Type\": \"application/json\"}\n",
        "    es_endpoint = f\"https://discoveryengine.googleapis.com/v1/projects/{project_id}/locations/{location}/collections/default_collection/engines?engineId={app_id}\"\n",
        "    response = requests.post(es_endpoint, data=json.dumps(payload), headers=header)\n",
        "    if response.status_code == 200:\n",
        "        print(f\"The creation of App {app_id}  is initiated.\")\n",
        "        print(\"It may take a few minutes for the App to become available\")\n",
        "    else:\n",
        "        print(f\"Failed to create App {app_id}\")\n",
        "        print(response.json())\n",
        "    return response.status_code"
      ],
      "metadata": {
        "id": "SxR3hw5Tke-q"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Datastore with the provided ID if it doesn't exist"
      ],
      "metadata": {
        "id": "1hAp5cBnIYxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if datastore_exists(PROJECT_ID, LOCATION, DATASTORE_ID):\n",
        "    print(f\"Datastore {DATASTORE_ID} already exists.\")\n",
        "else:\n",
        "    create_datastore(PROJECT_ID, LOCATION, DATASTORE_ID)"
      ],
      "metadata": {
        "id": "hBUwJxxeAazj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Optional] Check if the Datastore is created successfully\n",
        "\n",
        "\n",
        "The Datastore is polled to track when it becomes available.\n",
        "\n",
        "This may take a few minutes"
      ],
      "metadata": {
        "id": "C1d-pd2WLJZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while not datastore_exists(PROJECT_ID, LOCATION, DATASTORE_ID):\n",
        "    print(f\"Datastore {DATASTORE_ID} is still being created.\")\n",
        "    time.sleep(30)\n",
        "print(f\"Datastore {DATASTORE_ID} created successfully.\")"
      ],
      "metadata": {
        "id": "EZGzOCnTLOwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create an App with the provided ID if it doesn't exist\n",
        "The App will be connected to a Datastore with the provided ID earlier in this notebook"
      ],
      "metadata": {
        "id": "vSzz2AzmI5kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if app_exists(PROJECT_ID, LOCATION, APP_ID):\n",
        "    print(f\"App {APP_ID} already exists.\")\n",
        "else:\n",
        "    create_app(PROJECT_ID, LOCATION, DATASTORE_ID, APP_ID)\n"
      ],
      "metadata": {
        "id": "4lp4kPXNm9sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Optional] Check if the App is created successfully\n",
        "\n",
        "\n",
        "The App is polled to track when it becomes available.\n",
        "\n",
        "This may take a few minutes"
      ],
      "metadata": {
        "id": "fxlTn7dVK-Q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while not app_exists(PROJECT_ID, LOCATION, APP_ID):\n",
        "    print(f\"App {APP_ID} is still being created.\")\n",
        "    time.sleep(30)\n",
        "print(f\"App {APP_ID} created successfully.\")"
      ],
      "metadata": {
        "id": "ZuQQ2HCGK4BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Optional] Provide your own Schema\n",
        "\n",
        " The schema is detected automatically but it can be optionally adjusted to decide which fields should be:\n",
        "\n",
        " - Retrievable (returned in the response),\n",
        " - Searchable (searched through term-based and semantically),\n",
        " - Indexible (filtered, boosted etc)\n",
        "\n",
        "We can also specify keyProperties which gives special retrieval treatment to certain fields.\n",
        "\n",
        "See this documentation on [auto-detecting versus providing your own Schema](https://cloud.google.com/generative-ai-app-builder/docs/provide-schema)"
      ],
      "metadata": {
        "id": "-7LR113gJg7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema: Dict[str, Any] = {\n",
        "    \"structSchema\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"doc_name\": {\n",
        "                \"keyPropertyMapping\": \"title\",\n",
        "                \"retrievable\": True,\n",
        "                \"dynamicFacetable\": False,\n",
        "                \"type\": \"string\"\n",
        "            },\n",
        "            \"year\": {\n",
        "                \"retrievable\": True,\n",
        "                \"indexable\": True,\n",
        "                \"dynamicFacetable\": False,\n",
        "                \"searchable\": False,\n",
        "                \"type\": \"string\"\n",
        "            },\n",
        "            \"quarter\": {\n",
        "                \"retrievable\": True,\n",
        "                \"indexable\": True,\n",
        "                \"dynamicFacetable\": False,\n",
        "                \"searchable\": False,\n",
        "                \"type\": \"string\"\n",
        "            },\n",
        "            \"doc_type\": {\n",
        "                \"retrievable\": True,\n",
        "                \"indexable\": True,\n",
        "                \"dynamicFacetable\": False,\n",
        "                \"searchable\": False,\n",
        "                \"type\": \"string\"\n",
        "            },\n",
        "            \"stock_tickers\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                  \"type\": \"string\",\n",
        "                  \"keyPropertyMapping\": \"category\"\n",
        "                }\n",
        "            },\n",
        "            \"company_name\": {\n",
        "                \"retrievable\": True,\n",
        "                \"indexable\": True,\n",
        "                \"dynamicFacetable\": False,\n",
        "                \"searchable\": False,\n",
        "                \"type\": \"string\"\n",
        "            },\n",
        "        },\n",
        "        \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.patch(\n",
        "    f'https://discoveryengine.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/collections/default_collection/dataStores/{DATASTORE_ID}/schemas/default_schema',\n",
        "    headers={\n",
        "        'Content-Type': 'application/json',\n",
        "        'Authorization': REST_TOKEN,\n",
        "    },\n",
        "    json = schema,\n",
        ")\n",
        "print(response.json())\n",
        "schema_update_lro = response.json()[\"name\"]"
      ],
      "metadata": {
        "id": "aO13rwu1Q6jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the status of Schema update\n",
        "\n",
        "For an empty Datastore the Schema update should be almost instantaneous.\n",
        "\n",
        "A request to update the schema creates a [Long-Running Operation](https://cloud.google.com/generative-ai-app-builder/docs/long-running-operations) which can be polled."
      ],
      "metadata": {
        "id": "EeuAlWHJKZ2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    response = requests.get(\n",
        "        f\"https://discoveryengine.googleapis.com/v1/{schema_update_lro}\",\n",
        "        headers={\"Authorization\": REST_TOKEN},\n",
        "    )\n",
        "    try:\n",
        "        status = response.json()[\"done\"]\n",
        "        if status:\n",
        "            print(f\"Import completed!\")\n",
        "            break\n",
        "    except:\n",
        "        print(f\"Import in progress.\")\n",
        "        time.sleep(10)"
      ],
      "metadata": {
        "id": "om_a4O4NV-dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Optional] Get the current Schema\n",
        "This block can be used to check whether or not the schema is in the desired state (particularly useful for an auto-detected schema)."
      ],
      "metadata": {
        "id": "Mr3n2jbaLoo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp = requests.get(\n",
        "    f'https://discoveryengine.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/collections/default_collection/dataStores/{DATASTORE_ID}/schemas/default_schema',\n",
        "    headers={\n",
        "        'Content-Type': 'application/json',\n",
        "        'Authorization': REST_TOKEN,\n",
        "    },\n",
        ")\n",
        "resp.json()"
      ],
      "metadata": {
        "id": "Ueel8I0NS66p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the path to documents and Metadata (both in GCS and Local)\n",
        "The JSONL GCS Directory will be used to store the JSONL file to-be-cereated. If such a directory does not exist, it will be created.\n",
        "\n",
        "For the purpose of this demo, the documents and their correponding metadata are joined based on the FIELD_FOR_FILE_NAME within the metadata (doc_name in this example)\n",
        "\n",
        "Based on that convention, the metadata for \"2022_Q1_Earnings_Transcript.pdf\" will have the following content:\n",
        "\n",
        "```json\n",
        " {\n",
        "     \"doc_name\": \"2022_Q1_Earnings_Transcript\",\n",
        "     \"year\": \"2022\",\n",
        "     \"quarter\": \"Q1\",\n",
        "     \"doc_type\": \"earnings transcript\",\n",
        "     \"stock_tickers\": [\"GOOG\", \"GOOGL\"],\n",
        "     \"company_name\": \"alphabet\",\n",
        " }\n",
        " ```\n",
        "\n",
        "The logic is applied for illustration purposes and you can apply any other joining logic that fits your data (e.g. common name between metadata and document files)"
      ],
      "metadata": {
        "id": "xoq4nUxyPuF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOCUMENT_FORMAT = 'pdf'  # @param [\"docx\", \"pdf\"]\n",
        "GCS_DIRECTORY_JSONL = ''  # @param {type:\"string\"}\n",
        "FIELD_FOR_FILE_NAME = \"doc_name\" # @param {type:\"string\"}\n",
        "\n",
        "JSONL_FILENAME = \"alphabet_earnings.json\"\n",
        "LOCAL_DOCS_PATH = \"data\"\n",
        "LOCAL_METADATA_PATH = \"metadata\"\n",
        "LOCAL_JSONL_PATH = \"jsonl\""
      ],
      "metadata": {
        "id": "s8YpjxkrprRg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper function to prepare JSONL content\n",
        "A JSONL file needs to be created which contains a joined list of docuemnts to be ingested and their metadata. You can find more details on the expected formatting [here](https://cloud.google.com/generative-ai-app-builder/docs/prepare-data#storage-unstructured)"
      ],
      "metadata": {
        "id": "_KGV_xyI1fxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_jsonl(row: pd.Series) -> Dict[str, Any]:\n",
        "    \"\"\"Prepares metadata for a given row in the DataFrame.\"\"\"\n",
        "    mimetype = 'application/vnd.openxmlformats-officedocument.wordprocessingml.document' if DOCUMENT_FORMAT == 'docx' else 'application/pdf'\n",
        "    struct_data = row.to_dict()\n",
        "    return {\n",
        "        \"id\": row[FIELD_FOR_FILE_NAME],\n",
        "        \"structData\": struct_data,\n",
        "        \"content\": {\"mimeType\": mimetype, \"uri\": f'{GCS_DIRECTORY_DOCS}{row[FIELD_FOR_FILE_NAME]}.{DOCUMENT_FORMAT}'}\n",
        "    }"
      ],
      "metadata": {
        "id": "qwpDRUWK1gEY"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare JSONL file and save to GCS\n",
        "Documents and their metadata are copied to the local path, loaded in a DataFrame, and processed to prepare a JSONL file with the expected format\n",
        "The JSONL file is then uploaded the provided GCS path"
      ],
      "metadata": {
        "id": "ZrJHYvSO11EG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy files from GCS to local\n",
        "os.makedirs(LOCAL_DOCS_PATH, exist_ok=True)\n",
        "os.makedirs(LOCAL_METADATA_PATH, exist_ok=True)\n",
        "os.makedirs(LOCAL_JSONL_PATH, exist_ok=True)\n",
        "!gsutil -m cp -r {GCS_DIRECTORY_DOCS}* {LOCAL_DOCS_PATH}\n",
        "!gsutil -m cp -r {GCS_DIRECTORY_METADATA}* {LOCAL_METADATA_PATH}\n",
        "\n",
        "# Load and process metadata\n",
        "metadata_files = glob.glob(f\"{os.getcwd()}/{LOCAL_METADATA_PATH}/*.txt\")\n",
        "df_json = pd.concat([pd.read_json(file, typ=\"series\") for file in metadata_files], axis=1).T  # Load all JSON into one DataFrame\n",
        "\n",
        "# Apply metadata preparation and save as JSONL\n",
        "df_json['metadata'] = df_json.apply(prepare_jsonl, axis=1)\n",
        "df_json['metadata'].to_json(f'{LOCAL_JSONL_PATH}/{JSONL_FILENAME}', orient='records', lines=True)\n",
        "\n",
        "# Upload the local JSONL file to GCS\n",
        "!gsutil -m cp {LOCAL_JSONL_PATH}/* {GCS_DIRECTORY_JSONL}\n",
        "\n",
        "# Optional print of the jsonL content\n",
        "print(\"\\nJSONL Content:\")\n",
        "for metadata_entry in df_json['metadata']:\n",
        "    print(json.dumps(metadata_entry, indent=2))"
      ],
      "metadata": {
        "id": "t8i9eB7G10ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import documents with metadata from JSONL on GCS\n",
        "This is where the actual import to the Datastore happens.\n",
        "The process is done Async, and the request returns an instance of a \"Long running Operation\"\n",
        "\n",
        "This may take xx minutes. Feel free to grab a coffee."
      ],
      "metadata": {
        "id": "x7cW0g5t2DSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def import_documents_from_gcs_jsonl(project_id: str, location: str, datastore_id: str, gcs_uri: str) -> str:\n",
        "    \"\"\"Imports documents from a JSONL file in GCS.\"\"\"\n",
        "    payload = {\n",
        "        \"reconciliationMode\": \"INCREMENTAL\",\n",
        "        \"gcsSource\": {\"inputUris\": [gcs_uri]},\n",
        "    }\n",
        "    header = {\"Authorization\": REST_TOKEN, \"Content-Type\": \"application/json\"}\n",
        "    es_endpoint = f\"https://discoveryengine.googleapis.com/v1/projects/{project_id}/locations/{location}/collections/default_collection/dataStores/{datastore_id}/branches/default_branch/documents:import\"\n",
        "    response = requests.post(es_endpoint, data=json.dumps(payload), headers=header)\n",
        "    print(f\"--{response.json()}\")\n",
        "    return response.json()[\"name\"]\n",
        "\n",
        "import_lro = import_documents_from_gcs_jsonl(\n",
        "    project_id=PROJECT_ID,\n",
        "    location=LOCATION,\n",
        "    datastore_id=DATASTORE_ID,\n",
        "    gcs_uri=f'{GCS_DIRECTORY_JSONL}{JSONL_FILENAME}',\n",
        ")"
      ],
      "metadata": {
        "id": "-FVcu7wGJcom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Optional] Check the status of document import via polling\n",
        "Optionally check the status of the long running operation for the import job. You can check this in the UI as well by looking at the \"activity\" tab of the corresponding Datastore"
      ],
      "metadata": {
        "id": "vttgvfVB2M-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    response = requests.get(\n",
        "        f\"https://discoveryengine.googleapis.com/v1/{import_lro}\",\n",
        "        headers={\"Authorization\": REST_TOKEN},\n",
        "    )\n",
        "    try:\n",
        "        status = response.json()[\"done\"]\n",
        "        if status:\n",
        "            print(f\"Import completed!\")\n",
        "            break\n",
        "    except KeyError:\n",
        "        print(f\"Import in progress.\")\n",
        "        time.sleep(60)"
      ],
      "metadata": {
        "id": "rYq_P_hDnSrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample search with filter on a single document\n",
        "The applied filter is specific to the examples in this notebook\n",
        "\n",
        "You can run w/wo the filter to see how it influences the reslts. Without the filte you get a mix of results dfrom different sources but with the filter you only get results from a single document.\n",
        "\n",
        "This block is for demonstration only, you can find more information on querying a Datastore [here](https://cloud.google.com/generative-ai-app-builder/docs/preview-search-results)"
      ],
      "metadata": {
        "id": "xn7DSXTD2XCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = requests.post(\n",
        "  f'https://discoveryengine.googleapis.com/v1alpha/projects/{PROJECT_ID}/locations/{LOCATION}/collections/default_collection/dataStores/{DATASTORE_ID}/servingConfigs/default_search:search',\n",
        "  headers={\n",
        "    'Content-Type': 'application/json',\n",
        "    'Authorization': REST_TOKEN,\n",
        "  },\n",
        "json = {\n",
        "      \"query\": \"Google revenue\",\n",
        "      \"filter\": 'quarter: ANY(\"Q2\")',\n",
        "}\n",
        "    )\n",
        "response.json()"
      ],
      "metadata": {
        "id": "TAhobXOEWvra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
